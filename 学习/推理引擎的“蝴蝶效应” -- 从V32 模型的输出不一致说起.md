# 摘要
本文确认了 DeepSeek-V32 模型在输入长度为 4K的场景中 KV缓存 不一致并非输入或写入错误，而是稀疏化注意力(Sparse Atttenion)的输入的索引顺序不一致引发的输出差异，且该差异可在最小复现脚本中稳定复现。

# 背景
起因是排查推理引擎的一个异常现象：同一个长请求，在不同 DP(Data Parallel) 节点上得到的部分KV 缓存 ( KV Cache ) 不一致，在我们刻意关闭 batch 干扰、仅保留单请求和并行度 1 的情况下仍然复现，这个现象很不自然，于是我们决定把这条链路拆开，一段一段去验证精度问题。

# 实验方法和结果
针对 DeepSeek 模型，当前的计算逻辑可以简单拆分为 `attn0`（`q_a/b_proj, bmm_wkc, kv_a_proj, rotary_emb`计算）、`attn1`（`mqa, bmm_wkv, o_proj` 计算） 和混合专家 (MoE) 三大部分，因此实验的第一步是对每一层的 `attn0` 与 `attn1` 输出进行导出并做严格对比。
![[attn0_dump_code.png|attn0 的导出代码]]

![[attn1_dump_code.png|attn1的导出代码]]

结果显示第 0 层的 `attn0` 可以做到 每个比特级别 (bitwise) 一致，而 `attn1` 不能做到完全一致，这一步直接把我们的归因的注意力集中到 `attn1` 内部的稀疏化注意力输出。为了让方法可复现，我们使用 `diff_safetensors.py` 对每一层的 `attn0` 与 `attn1` 输出做逐项对比，必要时输出差异窗口与直方图，脚本细节见附录。
![[attn0_layer_0_compare.png|layer0的 attn0 的结果对比]]

![[attn1_layer_0_compare.png|attn1 的 layer0 的输出结果对比]]

可以看到，对比结果显示attn1 的计算部分中，主要的不一样有三点：`attn_bmm_output`，`attn_output`, 以及最后的 `output` 不一致。而我们已知：`attn1` 计算部分中，最开始的计算就是`flashMLA` 的 `Sparse Attntion`，它的输出不一致，显然会造成后续所有算子输出的不一致。

因此接着我们继续追溯 `Sparse Attention` 的输入，输入路径包含 `QKV、kvcache` 以及 `index`，其中 `QKV` 经过逐项对比保持一致，因此问题不在 `QKV`，`kvcache` 需要通过 `pagetable_1` 索引取得，我们将 `pagetable_1` 排序后再按排序后的索引去取 `kvcache`，结果可以完全对齐，这说明 `kvcache` 内容一致但顺序不同，问题进一步收敛到 `index` 的差异。为了排除索引顺序差异只是表象这一可能性，我们对 `pagetable_1` 的索引集合进行严格比较，并验证排序后 `kv_cache_by_pagetable` 一致，从而确定差异来自 `index` 顺序而非索引集合，这一步把问题从写入路径排除后稳定落在索引顺序上。
![[kvcache_sorted_compare.png|kvcache 排序后严格相等]]

最后一步是最小复现，我们使用 `test_attn_mqa_determinism.py`脚本 (具体代码见附录)，保持 `topk` 集合不变，仅交换 `index` 顺序并在多个 GPU 上重复运行，结果稳定出现 BF16 量级差异，这说明 `index` 顺序变化足以引发 `Sparse_Attention` 输出差异，进而导致 `attn1` 输出不一致，以下为一次执行的 `shell` 输出。

```text
run=0 device=cuda:0 same=False max_diff=0.00390625 same_bmm=False max_diff_bmm=0.0625
run=1 device=cuda:0 same=False max_diff=0.00390625 same_bmm=False max_diff_bmm=0.09375
run=2 device=cuda:0 same=False max_diff=0.00390625 same_bmm=False max_diff_bmm=0.125
run=3 device=cuda:0 same=False max_diff=0.00390625 same_bmm=False max_diff_bmm=0.0625
run=4 device=cuda:0 same=False max_diff=0.00390625 same_bmm=False max_diff_bmm=0.0625
run=5 device=cuda:0 same=False max_diff=0.00390625 same_bmm=False max_diff_bmm=0.09375
run=6 device=cuda:0 same=False max_diff=0.00390625 same_bmm=False max_diff_bmm=0.0625
run=7 device=cuda:0 same=False max_diff=0.00390625 same_bmm=False max_diff_bmm=0.0625
```

# 结论
问题的根因是稀疏化注意力输入的索引顺序不一致，虽然索引集合一致但顺序不同，导致注意力模块计算放大成输出差异，随着每经过一层的 `topk` 和注意力计算，误差逐步累加，最后输出的差异就如下图对比的结果一样差之千里。因此我们得出结论：对于 DeepSeek-V32 这类使用稀疏化注意力的模型，其面对相同的输入，输出本身就会存在差异，且随着层数的增多，逐步形成肉眼可观测到的差异。
![[逐层误差区间对比直方图.png|两个一样的 req 的逐层输出结果误差范围对比]]

# 进一步探索

这一步可以把原因拆成两层，第一层是 `topk` 的稳定性问题，`topk` 在 GPU 上为了吞吐通常使用分块筛选与并行归约，尤其在 `score` 相等或近似相等时无法保证稳定排序，索引集合是一致的但顺序可能改变，这是性能优先带来的可接受不确定性。
第二层是 attn 的数值确定性问题，softmax 的稳定形式是先取最大值再归一化，但是在 GPU 上进行加速计算时，会选取online-softmax , 在合并两个块的结果时，会用到如下的合并公式 ：
$$
\text{m}=\max(m_A, m_B)) \tag{1}
$$
$$
\text{s}= (exp(m_A-m) \cdot s_A+ \exp(m_B-m) \cdot s_B) \tag{2}
$$

有限精度下加法非结合导致不同块顺序出现不同舍入轨迹，所以顺序变化会改变最终输出，这属于数学算法在有限精度下的数值特性而非功能缺陷。具体到这次的 index 顺序变化，会导致每次计算 online-softmax 时，每次合并的这一步算法，更新的 s 的值不一致，导致了在 BF16 上的精度差异。


# 附录
下面给出复现与对比用到的脚本，便于完整复现和验证。
## diff_safetensors.py
```python
#!/usr/bin/env python3

import argparse
import glob
import os
import re

import matplotlib
matplotlib.use('Agg')
import matplotlib.pyplot as plt
import torch
from safetensors.torch import load_file
from rich.console import Console
from rich.table import Table

HIST_BINS = 100
HIST_MODE = 'abs'
HIST_DIR = '.'
HIST_MAX_QUANTILE = 0.99
HIST_MAX_SAMPLE = 5_000_000


def parse_args():
    parser = argparse.ArgumentParser(description='Compare safetensors dumps per tp rank.')
    parser.add_argument('--dir-a', required=True, help='dir for ckpt A')
    parser.add_argument('--dir-b', required=True, help='dir for ckpt B')
    parser.add_argument('--layer-id', type=int, default=0, help='layer id in file name')
    parser.add_argument(
        '--section',
        choices=(
            'attn0',
            'attn1',
            'kvcache',
            'pagetable_input',
            'kvcache_write',
            'kvcache_write_back',
            'kvcache_flow',
        ),
        default='attn1',
        help='select dump section',
    )
    parser.add_argument('--rtol', type=float, default=1e-5)
    parser.add_argument('--atol', type=float, default=1e-2)
    parser.add_argument('--window', type=int, default=50)
    parser.add_argument('--max-mismatch', type=int, default=1)
    return parser.parse_args()


def sort_topk(x):
    return torch.sort(x, dim=-1)[0]


def _is_close_mask(a, b, rtol, atol):
    if a.is_floating_point() or b.is_floating_point():
        return torch.isclose(a, b, rtol=rtol, atol=atol)
    return a == b


def show_mismatch_window(a, b, key, rtol, atol, window, max_mismatch, console):
    a = a.reshape(-1)
    b = b.reshape(-1)
    close_mask = _is_close_mask(a, b, rtol, atol)
    mismatch = ~close_mask
    idxs = torch.nonzero(mismatch).flatten()
    if idxs.numel() == 0:
        return

    for idx in idxs[:max_mismatch].tolist():
        start = max(0, idx - window)
        end = min(a.numel(), idx + window + 1)

        table = Table(title=f'{key} mismatch at {idx} (window {start}:{end})')
        table.add_column('idx')
        table.add_column('a')
        table.add_column('b')
        table.add_column('abs_diff')

        for i in range(start, end):
            av = a[i]
            bv = b[i]
            diff = (av - bv).abs()
            table.add_row(
                str(i),
                f'{av.item():.6g}',
                f'{bv.item():.6g}',
                f'{diff.item():.6g}',
            )
        console.print(table)


def compare_rank(ckpt_a, ckpt_b, args, console, file_tag):
    topk_a = ckpt_a.get('topk_indices')
    topk_b = ckpt_b.get('topk_indices')
    for key in ckpt_a.keys():
        if key == 'topk_indices':
            continue
        a = ckpt_a[key]
        b = ckpt_b[key]

        if a.is_floating_point() or b.is_floating_point():
            is_close = torch.allclose(a, b, rtol=args.rtol, atol=args.atol)
        else:
            is_close = torch.equal(a, b)
        print(f'{key} allclose: {is_close}')
        if not is_close:
            _save_diff_histogram(a, b, key, file_tag)
            show_mismatch_window(a, b, key, args.rtol, args.atol, args.window, args.max_mismatch, console)
    return topk_a, topk_b


def _tp_rank_from_name(name):
    match = re.search(r'_tp(\d+)\.safetensors$', name)
    if match:
        return int(match.group(1))
    return -1


def _prepare_topk(tensor):
    if tensor.ndim == 0:
        tensor = tensor.reshape(1)
    if tensor.ndim == 1:
        tensor = tensor.reshape(1, -1)
    return sort_topk(tensor)


def _safe_filename(name):
    return re.sub(r'[^A-Za-z0-9_.-]+', '_', name)


def _build_kvcache_flow(ckpt_kvcache, ckpt_write_back):
    page_table = ckpt_kvcache['page_table_1']
    cache_loc = ckpt_write_back['cache_loc']
    cache_loc_unique = torch.unique(cache_loc.reshape(-1))
    hit_mask = torch.isin(page_table, cache_loc_unique)
    hit_count = hit_mask.sum(dim=-1)
    valid_count = (page_table >= 0).sum(dim=-1)
    return {
        'page_table_hit_count': hit_count,
        'page_table_valid_count': valid_count,
    }


def _sorted_valid_indices(x: torch.Tensor) -> torch.Tensor:
    flat = x.reshape(-1)
    flat = flat[flat >= 0]
    if flat.numel() == 0:
        return flat
    return torch.sort(flat)[0]


def _sort_pagetable_kv(
    page_table: torch.Tensor, kv_cache_by_pagetable: torch.Tensor
) -> tuple[torch.Tensor, torch.Tensor]:
    sort_dim = page_table.dim() - 1
    pad_value = torch.iinfo(page_table.dtype).max
    page_table_work = page_table.clone()
    page_table_work[page_table_work < 0] = pad_value
    sorted_page_table, sort_idx = torch.sort(page_table_work, dim=sort_dim)
    sorted_page_table = sorted_page_table.clone()
    sorted_page_table[sorted_page_table == pad_value] = -1
    idx = sort_idx
    for _ in range(kv_cache_by_pagetable.dim() - sort_idx.dim()):
        idx = idx.unsqueeze(-1)
    idx = idx.expand(*sort_idx.shape, *kv_cache_by_pagetable.shape[sort_idx.dim():])
    sorted_kv = torch.gather(kv_cache_by_pagetable, dim=sort_dim, index=idx)
    return sorted_page_table, sorted_kv


def _pad_1d(x: torch.Tensor, size: int, pad_value: int) -> torch.Tensor:
    if x.numel() >= size:
        return x
    pad = torch.full((size - x.numel(),), pad_value, dtype=x.dtype)
    return torch.cat([x, pad], dim=0)


def _maybe_sample(x: torch.Tensor) -> torch.Tensor:
    if x.numel() <= HIST_MAX_SAMPLE:
        return x
    idx = torch.randint(0, x.numel(), (HIST_MAX_SAMPLE,), device=x.device)
    return x[idx]


def _save_diff_histogram(a, b, key, file_tag):
    diff = a - b
    if HIST_MODE == 'abs':
        diff = diff.abs()
    diff = diff.reshape(-1).to(dtype=torch.float32)
    if HIST_MODE == 'abs':
        min_val = 0.0
        non_zero = diff > 0
        if torch.any(non_zero):
            sample = _maybe_sample(diff[non_zero])
            max_val = torch.quantile(sample, HIST_MAX_QUANTILE).item()
        else:
            max_val = diff.max().item()
    else:
        min_val = diff.min().item()
        max_val = diff.max().item()
    if min_val == max_val:
        min_val -= 1e-6
        max_val += 1e-6
    hist = torch.histc(diff, bins=HIST_BINS, min=min_val, max=max_val)
    bin_edges = torch.linspace(min_val, max_val, HIST_BINS + 1)
    centers = (bin_edges[:-1] + bin_edges[1:]) / 2
    width = (bin_edges[1] - bin_edges[0]).item()

    plt.figure(figsize=(8, 4))
    plt.bar(centers.numpy(), hist.numpy(), width=width)
    plt.xlabel('diff')
    plt.ylabel('count')
    plt.title(f'{file_tag}:{key} diff histogram ({HIST_MODE})')

    safe_key = _safe_filename(key)
    safe_tag = _safe_filename(file_tag)
    out_name = f'{safe_tag}__{safe_key}__diff_hist.png'
    out_path = os.path.join(HIST_DIR, out_name)
    plt.tight_layout()
    plt.savefig(out_path)
    plt.close()


def main():
    args = parse_args()
    console = Console()
    os.makedirs(HIST_DIR, exist_ok=True)

    if args.section == 'kvcache_flow':
        pattern = f'debug_attn1_kvcache_file_{args.layer_id}_tp*.safetensors'
        files_a = glob.glob(os.path.join(args.dir_a, pattern))
        files_a = sorted(
            files_a, key=lambda path: _tp_rank_from_name(os.path.basename(path))
        )
        for file_a in files_a:
            name = os.path.basename(file_a)
            console.rule(name)
            file_b = os.path.join(args.dir_b, name)
            file_a_write = os.path.join(
                args.dir_a, name.replace('kvcache_file', 'kvcache_write_back_file')
            )
            file_b_write = os.path.join(
                args.dir_b, name.replace('kvcache_file', 'kvcache_write_back_file')
            )
            ckpt_a = load_file(file_a)
            ckpt_b = load_file(file_b)
            write_a = load_file(file_a_write)
            write_b = load_file(file_b_write)
            derived_a = _build_kvcache_flow(ckpt_a, write_a)
            derived_b = _build_kvcache_flow(ckpt_b, write_b)
            page_a = _sorted_valid_indices(ckpt_a['page_table_1'])
            page_b = _sorted_valid_indices(ckpt_b['page_table_1'])
            cache_a = _sorted_valid_indices(write_a['cache_loc'])
            cache_b = _sorted_valid_indices(write_b['cache_loc'])
            page_size = max(page_a.numel(), page_b.numel())
            cache_size = max(cache_a.numel(), cache_b.numel())
            derived_a['page_table_sorted_indices'] = _pad_1d(page_a, page_size, -1)
            derived_b['page_table_sorted_indices'] = _pad_1d(page_b, page_size, -1)
            derived_a['cache_loc_sorted_indices'] = _pad_1d(cache_a, cache_size, -1)
            derived_b['cache_loc_sorted_indices'] = _pad_1d(cache_b, cache_size, -1)
            compare_rank(derived_a, derived_b, args, console, name)
        return

    if args.section == 'attn0':
        pattern = f'debug_attn0_tensor_file_{args.layer_id}_tp*.safetensors'
    elif args.section == 'attn1':
        pattern = f'debug_attn_mlp_out_tensor_file_{args.layer_id}_tp*.safetensors'
    elif args.section == 'pagetable_input':
        pattern = f'debug_attn1_pagetable_input_file_{args.layer_id}_tp*.safetensors'
    elif args.section == 'kvcache_write':
        pattern = f'debug_attn1_kvcache_write_file_{args.layer_id}_tp*.safetensors'
    elif args.section == 'kvcache_write_back':
        pattern = f'debug_attn1_kvcache_write_back_file_{args.layer_id}_tp*.safetensors'
    else:
        pattern = f'debug_attn1_kvcache_file_{args.layer_id}_tp*.safetensors'
    files_a = glob.glob(os.path.join(args.dir_a, pattern))
    files_a = sorted(files_a, key=lambda path: _tp_rank_from_name(os.path.basename(path)))

    topk_list_a = []
    topk_list_b = []
    for file_a in files_a:
        name = os.path.basename(file_a)
        console.rule(name)
        file_b = os.path.join(args.dir_b, name)
        ckpt_a = load_file(file_a)
        ckpt_b = load_file(file_b)
        if args.section == 'kvcache':
            page_a, kv_a = _sort_pagetable_kv(
                ckpt_a['page_table_1'], ckpt_a['kv_cache_by_pagetable']
            )
            page_b, kv_b = _sort_pagetable_kv(
                ckpt_b['page_table_1'], ckpt_b['kv_cache_by_pagetable']
            )
            ckpt_a = {
                'page_table_1_sorted': page_a,
                'kv_cache_by_pagetable_sorted': kv_a,
            }
            ckpt_b = {
                'page_table_1_sorted': page_b,
                'kv_cache_by_pagetable_sorted': kv_b,
            }
        topk_a, topk_b = compare_rank(ckpt_a, ckpt_b, args, console, name)
        if topk_a is not None:
            topk_list_a.append(_prepare_topk(topk_a))
        if topk_b is not None:
            topk_list_b.append(_prepare_topk(topk_b))

    if topk_list_a and topk_list_b:
        merged_topk_a = torch.cat(topk_list_a, dim=0)
        merged_topk_b = torch.cat(topk_list_b, dim=0)
        is_equal = torch.equal(merged_topk_a, merged_topk_b)
        print(f'topk_indices allclose(sorted, all tp): {is_equal}')
        if not is_equal:
            show_mismatch_window(
                merged_topk_a,
                merged_topk_b,
                'topk_indices_all_tp',
                args.rtol,
                args.atol,
                args.window,
                args.max_mismatch,
                console,
            )


if __name__ == '__main__':
    main()
```

## test_attn_mqa_determinism.py
```python
import math

import torch

H_KV = 1
QK_ROPE_HEAD_DIM = 64


def _build_sparse_indices(
    bs: int,
    s_q: int,
    h_kv: int,
    topk: int,
    kv_len: int,
    device: str,
    rng: torch.Generator,
) -> torch.Tensor:
    total_q = bs * s_q
    total_kv = bs * kv_len
    indices = torch.full(
        (total_q, h_kv, topk),
        total_kv,
        device=device,
        dtype=torch.int32,
    )
    for b in range(bs):
        kv_base = b * kv_len
        q_base = b * s_q
        for q in range(s_q):
            q_idx = q_base + q
            for h in range(h_kv):
                valid_k = min(topk, kv_len)
                near_mask = (
                    torch.randint(0, 32, (valid_k,), device=device, generator=rng) < 31
                )
                cur = torch.randperm(kv_len, generator=rng, device=device)[:valid_k]
                if near_mask.any():
                    cur[near_mask] = torch.randint(
                        max(0, kv_len - 20000),
                        kv_len,
                        (int(near_mask.sum().item()),),
                        device=device,
                        generator=rng,
                    )
                if valid_k < topk:
                    pad = torch.full(
                        (topk - valid_k,),
                        total_kv,
                        device=device,
                        dtype=torch.int32,
                    )
                    cur = torch.cat([cur, pad])
                indices[q_idx, h] = cur + kv_base
    return indices


def _run_once(run_id: int, device_id: int) -> None:
    device = f"cuda:{device_id}"
    torch.cuda.set_device(device)
    dtype = torch.bfloat16
    seed = 42
    batch_size = 1
    seq_len = 4096
    head_num = 128
    head_dim = 576
    topk = 2048
    v_head_dim = head_dim - QK_ROPE_HEAD_DIM
    sm_scale = 1.0 / math.sqrt(head_dim)

    rng = torch.Generator(device=device)
    rng.manual_seed(seed)
    q = torch.randn(
        (batch_size * seq_len, head_num, head_dim),
        device=device,
        dtype=dtype,
        generator=rng,
    )
    kv = torch.randn(
        (batch_size * seq_len, H_KV, head_dim),
        device=device,
        dtype=dtype,
        generator=rng,
    )
    indices = _build_sparse_indices(
        batch_size, seq_len, H_KV, topk, seq_len, device, rng
    )
    perm_rng = torch.Generator(device=device)
    perm_rng.manual_seed(seed + 1000 + run_id)
    order = torch.randperm(topk, generator=perm_rng, device=device)
    indices_perm = indices.index_select(-1, order)

    from flash_mla import flash_mla_sparse_fwd

    out1, _, _ = flash_mla_sparse_fwd(
        q=q,
        kv=kv,
        indices=indices,
        sm_scale=sm_scale,
        d_v=v_head_dim,
    )
    out2, _, _ = flash_mla_sparse_fwd(
        q=q,
        kv=kv,
        indices=indices_perm,
        sm_scale=sm_scale,
        d_v=v_head_dim,
    )

    rng_w = torch.Generator(device=device)
    rng_w.manual_seed(seed)
    w = torch.randn(
        (head_num, v_head_dim, v_head_dim),
        device=device,
        dtype=dtype,
        generator=rng_w,
    )
    out1_bmm = torch.bmm(out1.transpose(0, 1), w).transpose(0, 1)
    out2_bmm = torch.bmm(out2.transpose(0, 1), w).transpose(0, 1)

    same = torch.equal(out1, out2)
    max_diff = (out1 - out2).abs().max().item()
    same_bmm = torch.equal(out1_bmm, out2_bmm)
    max_diff_bmm = (out1_bmm - out2_bmm).abs().max().item()
    print(
        f"run={run_id} device={device} same={same} max_diff={max_diff} "
        f"same_bmm={same_bmm} max_diff_bmm={max_diff_bmm}"
    )


def main() -> None:
    runs = 8
    device_count = torch.cuda.device_count()
    for run_id in range(runs):
        device_id = run_id % device_count
        _run_once(run_id, device_id)


if __name__ == "__main__":
    main()
```
